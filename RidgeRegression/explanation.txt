「リッジ回帰」
特徴量の多いデータ分析、特徴量同士で相関関係がある場合に有効

(説明)
・リッジ回帰はバリアンスを低く調整することに焦点をおいて、バイアスが増えることを
妥協して、回帰係数を小さくすることが目的
・回帰係数の2乗の和を損失関数を用いて回帰係数を小さくする

・回帰係数が小さいと訓練データで過学習が起こりにくくなる。

(用語)
バイアス　→　予測データと実際のデータがどれくらい離れてるかを表す
バリアンス　→　訓練データに依存して過学習が起こってるかを表す

(線形回帰とリッジ回帰の判断)
・線形回帰　→　特徴データが少なくて、特徴データ同士で相関関係がない場合

・リッジ回帰　→　特徴データが多かったり、特徴データ同士で相関関係にあるとき

    (判断できないとき)
    ・線形回帰で訓練データ、検証データ、テストデータに分けて学習して、
    　過学習があるか確認して、過学習があった場合は、リッジ回帰などに切り替える
    　・リッジ回帰も検証データを使う方が良い
    
(リッジ回帰とラッソ回帰の判断)
・特徴量選択が重要な場合や、モデルの解釈性を重視する場合はラッソ回帰が適してる。
　全ての特徴量に何らかの寄与を期待する場合はリッジ回帰の方が向いている




